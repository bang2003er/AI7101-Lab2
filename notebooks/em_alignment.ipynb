{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q4SFgcwDo5e"
      },
      "source": [
        "# Alignment of EM sections\n",
        "\n",
        "This notebook shows how to use SOFIMA to elastically align ultrathin sections of brain tissue acquired with an electron microscope. We will be using a 5000x5000x200 subset of a SBEM dataset of zebrafish brain acquired in Rainer Friedrich's lab at the FMI. You can browse the [unaligned data](https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B1.1e-8%2C%22m%22%5D%2C%22y%22:%5B1.1e-8%2C%22m%22%5D%2C%22z%22:%5B2.5e-8%2C%22m%22%5D%7D%2C%22position%22:%5B2359.354248046875%2C2653.44775390625%2C99.5%5D%2C%22crossSectionScale%22:3.5180938094527145%2C%22projectionScale%22:8192%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://sofima-sample-data/fmi-friedrich-dp/subvol_5800_5500_6250%22%2C%22tab%22:%22source%22%2C%22name%22:%22subvol_5800_5500_6250%22%7D%5D%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22subvol_5800_5500_6250%22%7D%2C%22layout%22:%224panel%22%7D) in Neuroglancer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QA66vvq4VEyM",
        "outputId": "45a80f97-5205-479b-fd01-2d549b5b0725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RB14BypiDmM0",
        "outputId": "34f64da3-ad1b-4e29-a6f1-5d2f0ac82424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (0.1.79)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from tensorstore) (2.0.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorstore) (0.5.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorstore\n",
        "import tensorstore as ts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "storage_dir = \"/content/drive/MyDrive/batch_004_4096_1_ts\"  # 这里替换成你本地存储路径"
      ],
      "metadata": {
        "id": "5fk85PhQ0qcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WcQg7rblEYWE",
        "outputId": "9dae6637-2d68-427d-9ac1-e191be3ab57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error parsing object member \"kvstore\": Parsing spec from url: \"/content/drive/MyDrive/batch_004_4096_1_ts\": URL scheme must be specified in \"/content/drive/MyDrive/batch_004_4096_1_ts\" [source locations='tensorstore/kvstore/url_registry.cc:163\\ntensorstore/kvstore/url_registry.cc:163\\ntensorstore/kvstore/spec.cc:93\\ntensorstore/internal/json_binding/json_binding.h:873\\ntensorstore/internal/json_binding/json_binding.h:829']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3506864735.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m unaligned_1x = ts.open({\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'driver'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'neuroglancer_precomputed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'kvstore'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'/content/drive/MyDrive/batch_004_4096_1_ts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"scale_metadata\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"resolution\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"cache_pool\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"total_bytes_limit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1_000_000_000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error parsing object member \"kvstore\": Parsing spec from url: \"/content/drive/MyDrive/batch_004_4096_1_ts\": URL scheme must be specified in \"/content/drive/MyDrive/batch_004_4096_1_ts\" [source locations='tensorstore/kvstore/url_registry.cc:163\\ntensorstore/kvstore/url_registry.cc:163\\ntensorstore/kvstore/spec.cc:93\\ntensorstore/internal/json_binding/json_binding.h:873\\ntensorstore/internal/json_binding/json_binding.h:829']"
          ]
        }
      ],
      "source": [
        "# unaligned_1x = ts.open({\n",
        "#     'driver': 'neuroglancer_precomputed',\n",
        "#     'kvstore': 'gs://sofima-sample-data/fmi-friedrich-dp/subvol_5800_5500_6250',\n",
        "#     \"scale_metadata\": {\"resolution\": [11, 11, 25]},\n",
        "#     \"context\": { \"cache_pool\": {\"total_bytes_limit\": 1_000_000_000},}\n",
        "#     }).result()\n",
        "\n",
        "unaligned_1x = ts.open({\n",
        "    'driver': 'neuroglancer_precomputed',  # 使用 Neuroglancer Precomputed 驱动\n",
        "    'kvstore': f'file://{storage_dir}',  # 使用本地路径\n",
        "    'scale_metadata': {\"resolution\": [11, 11, 25]},  # 设置 s0 层级的分辨率\n",
        "    'context': {'cache_pool': {'total_bytes_limit': 1_000_000_000}},  # 设置缓存池\n",
        "}).result()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq5NMj3RICCk"
      },
      "outputs": [],
      "source": [
        "# Precomputed volume, 2x downsampled resolution. Downsampling was done with area averaging.\n",
        "# unaligned_2x = ts.open({\n",
        "#     'driver': 'neuroglancer_precomputed',\n",
        "#     'kvstore': 'gs://sofima-sample-data/fmi-friedrich-dp/subvol_5800_5500_6250',\n",
        "#     \"scale_metadata\": {\"resolution\": [22, 22, 25]},\n",
        "#     \"context\": { \"cache_pool\": {\"total_bytes_limit\": 1_000_000_000},}\n",
        "#     }).result()\n",
        "\n",
        "unaligned_2x = ts.open({\n",
        "    'driver': 'neuroglancer_precomputed',  # 使用 Neuroglancer Precomputed 驱动\n",
        "    'kvstore': f'file://{storage_dir}',  # 使用本地路径\n",
        "    'scale_metadata': {\"resolution\": [22, 22, 25]},  # 设置 s1 层级的分辨率\n",
        "    'context': {'cache_pool': {'total_bytes_limit': 1_000_000_000}},  # 设置缓存池\n",
        "}).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycBIhHe4ISoQ"
      },
      "outputs": [],
      "source": [
        "unaligned_1x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZvmtg0TTBAJ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/sofima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fs5x8oyJVz2"
      },
      "outputs": [],
      "source": [
        "from concurrent import futures\n",
        "import time\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from connectomics.common import bounding_box\n",
        "from sofima import flow_field\n",
        "from sofima import flow_utils\n",
        "from sofima import map_utils\n",
        "from sofima import mesh\n",
        "from sofima import warp\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmV01gjBT8Lj"
      },
      "outputs": [],
      "source": [
        "# Ensure that we're running this code on a GPU machine. If this fails and you're using\n",
        "# Google Colab, use \"Edit >> Notebook settings\" and set s\"Hardware accelerator\" to \"GPU\".\n",
        "assert jax.devices()[0].platform == 'gpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcv9Q5Xpt9Ob"
      },
      "source": [
        "# Flow field estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrnqDaixgbRA"
      },
      "source": [
        "First, we calculate the flow fields between the current section and the directly preceding section. Flow fields can also be computed between pairs of sections that are not directly adjacent. This is useful if sections are incomplete or missing, but is not something we have to worry about in this demo.\n",
        "\n",
        "In a distributed environment, this step would be done with the `EstimateFlow` processor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aM_PE9Ogi-e"
      },
      "source": [
        "On a V100, the expected time for the flow calculation over a single 5000x5000 section with the settings below, is ~0.6s. The patch (160) and step (40) sizes are set to conservative values which work for most synaptic-resolution EM volumes (i.e. at an in-plane resolution of ~10 nm/px).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6H4_twgeK68"
      },
      "outputs": [],
      "source": [
        "# Both of the settings below are expressed in pixels.\n",
        "patch_size = 160  # XY spatial context used for flow field estimation\n",
        "stride = 40  # XY distance between centers of adjacent patches.\n",
        "\n",
        "def _compute_flow(volume):\n",
        "  mfc = flow_field.JAXMaskedXCorrWithStatsCalculator()\n",
        "  flows = []\n",
        "  prev = volume[..., 0, 0].T.read().result()\n",
        "\n",
        "  fs = []\n",
        "  with futures.ThreadPoolExecutor() as tpe:\n",
        "    # Prefetch the next sections to memory so that we don't have to wait for them\n",
        "    # to load when the GPU becomes available.\n",
        "    for z in range(1, unaligned_1x.shape[2]):\n",
        "      fs.append(tpe.submit(lambda z=z: volume[..., z, 0].T.read().result()))\n",
        "\n",
        "    fs = fs[::-1]\n",
        "\n",
        "    for z in tqdm(range(1, unaligned_1x.shape[2])):\n",
        "      curr = fs.pop().result()\n",
        "\n",
        "      # The batch size is a parameter which impacts the efficiency of the computation (but\n",
        "      # not its result). It has to be large enough for the computation to fully utilize the\n",
        "      # available GPU capacity, but small enough so that the batch fits in GPU RAM.\n",
        "      flows.append(mfc.flow_field(prev, curr, (patch_size, patch_size),\n",
        "                                  (stride, stride), batch_size=256))\n",
        "      prev = curr\n",
        "\n",
        "  return flows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyqZFmt7gsFv"
      },
      "source": [
        "Compute flows at native and 2x reduced in-plane resolution. The lower resolution flow has reduced precision, but is helpful for providing approximate flow vectors in places where the full-resolution flow might be impossible to estimate, e.g. in the interior of cell bodies or blood vessels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdkvmxpFgq7_"
      },
      "outputs": [],
      "source": [
        "flows1x = np.array(_compute_flow(unaligned_1x))\n",
        "flows2x = np.array(_compute_flow(unaligned_2x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxsUs1aFg1RG"
      },
      "source": [
        "The flow fields generated in the previous step are 4-channel arrays, where the first two channels store the XY components of the flow vector, and the two remaining channels are measures of estimation quality (see `sofima.flow_field._batched_peaks` for more info)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2038eGfNP8iL"
      },
      "outputs": [],
      "source": [
        "# Convert to [channels, z, y, x].\n",
        "flows2x = np.transpose(flows2x, [1, 0, 2, 3])\n",
        "flows1x = np.transpose(flows1x, [1, 0, 2, 3])\n",
        "\n",
        "# Pad to account for the edges of the images where there is insufficient context to estimate flow.\n",
        "pad = patch_size // 2 // stride\n",
        "flows1x = np.pad(flows1x, [[0, 0], [0, 0], [pad, pad], [pad, pad]], constant_values=np.nan)\n",
        "flows2x = np.pad(flows2x, [[0, 0], [0, 0], [pad, pad], [pad, pad]], constant_values=np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MANYvPsUP_xs"
      },
      "source": [
        "We now remove uncertain flow estimates by replacing them with NaNs, and merge the two flow arrays into a single flow field at full resolution. In a distributed environment, this step would be done with the `ReconcileAndFilterFlows` processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV001fp_iPq5"
      },
      "outputs": [],
      "source": [
        "f1 = flow_utils.clean_flow(flows1x, min_peak_ratio=1.6, min_peak_sharpness=1.6, max_magnitude=80, max_deviation=20)\n",
        "f2 = flow_utils.clean_flow(flows2x, min_peak_ratio=1.6, min_peak_sharpness=1.6, max_magnitude=80, max_deviation=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6VOF38hlBnf"
      },
      "source": [
        "Plot the horizontal component of the flow vector, before (left) and after (right) filtering. While blobs indicate areas where uncertain flow estimates were removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e16DcG8Jkgop"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(5, 2.5))\n",
        "ax[0].imshow(flows1x[0, 14, ...], cmap=plt.cm.RdBu, vmin=-10, vmax=10)\n",
        "ax[1].imshow(f1[0, 14, ...], cmap=plt.cm.RdBu, vmin=-10, vmax=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wft0ADtsdunw"
      },
      "outputs": [],
      "source": [
        "from scipy import interpolate\n",
        "f2_hires = np.zeros_like(f1)\n",
        "\n",
        "scale = 0.5\n",
        "oy, ox = np.ogrid[:f2.shape[-2], :f2.shape[-1]]\n",
        "oy = oy.ravel() / scale\n",
        "ox = ox.ravel() / scale\n",
        "\n",
        "box1x = bounding_box.BoundingBox(start=(0, 0, 0), size=(f1.shape[-1], f1.shape[-2], 1))\n",
        "box2x = bounding_box.BoundingBox(start=(0, 0, 0), size=(f2.shape[-1], f2.shape[-2], 1))\n",
        "\n",
        "for z in tqdm(range(f2.shape[1])):\n",
        "  # Upsample and scale spatial components.\n",
        "  resampled = map_utils.resample_map(\n",
        "      f2[:, z:z + 1, ...],  #\n",
        "      box2x, box1x, 1 / scale, 1)\n",
        "  f2_hires[:, z:z + 1, ...] = resampled / scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SKQDT9TjzvX"
      },
      "outputs": [],
      "source": [
        "final_flow = flow_utils.reconcile_flows((f1, f2_hires), max_gradient=0, max_deviation=20, min_patch_size=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhRPJrMTQO22"
      },
      "outputs": [],
      "source": [
        "# Plot (left to right): high res. flow, upsampled low res. flow, combined flow to use for alignment.\n",
        "f, ax = plt.subplots(1, 3, figsize=(7.5, 2.5))\n",
        "ax[0].imshow(f1[0, 14, ...], cmap=plt.cm.RdBu, vmin=-10, vmax=10)\n",
        "ax[1].imshow(f2_hires[0, 14, ...], cmap=plt.cm.RdBu, vmin=-10, vmax=10)\n",
        "ax[2].imshow(final_flow[0, 14, ...], cmap=plt.cm.RdBu, vmin=-10, vmax=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngpXzbult_xB"
      },
      "source": [
        "# Mesh optimization\n",
        "\n",
        "We use an elastic mesh optimizer to find a configuration of the imagery that is compatible with the estimated flow field and preserves the original geometry as much as possible.\n",
        "\n",
        "The optimization proceeds sequentially, section by section. In a distributed environment, this computation can be parallelized across the plane (by independently solving overlapping XY tiles), as well as split into blocks along the Z axis. This makes it possible to scale this process to arbitrarily large volumes. For simplicity, here we solve the complete stack in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkf42uNJuJL3"
      },
      "outputs": [],
      "source": [
        "config = mesh.IntegrationConfig(dt=0.001, gamma=0.0, k0=0.01, k=0.1, stride=(stride, stride), num_iters=1000,\n",
        "                                max_iters=100000, stop_v_max=0.005, dt_max=1000, start_cap=0.01,\n",
        "                                final_cap=10, prefer_orig_order=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1-uw2ohutig"
      },
      "outputs": [],
      "source": [
        "solved = [np.zeros_like(final_flow[:, 0:1, ...])]\n",
        "origin = jnp.array([0., 0.])\n",
        "\n",
        "for z in tqdm(range(0, final_flow.shape[1])):\n",
        "  prev = map_utils.compose_maps_fast(final_flow[:, z:z+1, ...], origin, stride,\n",
        "                                     solved[-1], origin, stride)\n",
        "  x = np.zeros_like(solved[0])\n",
        "  x, e_kin, num_steps = mesh.relax_mesh(x, prev, config)\n",
        "  x = np.array(x)\n",
        "  solved.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev4OPAo8wuFj"
      },
      "outputs": [],
      "source": [
        "solved = np.concatenate(solved, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF6-Y3V6uCyq"
      },
      "source": [
        "# Image warping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Nlhnnnx2_k"
      },
      "source": [
        "Image warping requires an inverse coordinate map, so compute that first. In a distributed environment, this can be done with the `InvertMap` processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb54nkYpDIr5"
      },
      "outputs": [],
      "source": [
        "inv_map = map_utils.invert_map(solved, box1x, box1x, stride)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkbeWvtryNFK"
      },
      "source": [
        "We are now ready to render the aligned subvolume. To reduce RAM usage, we render a 1000x1000 part for all 200 sections.\n",
        "\n",
        "In a distributed environment, this can be done with the `WarpByMap` processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLapfgxDx1Sz"
      },
      "outputs": [],
      "source": [
        "warped = [np.transpose(unaligned_1x[1000:2000, 2000:3000, 0:1, 0].read().result(), [2, 1, 0])]\n",
        "\n",
        "for z in tqdm(range(1, unaligned_1x.shape[2])):\n",
        "  data_box = bounding_box.BoundingBox(start=(500, 1500, 0), size=(2000, 2000, 1))\n",
        "  out_box = bounding_box.BoundingBox(start=(1000, 2000, 0), size=(1000, 1000, 1))\n",
        "\n",
        "  data = np.transpose(unaligned_1x[data_box.start[0]:data_box.end[0],\n",
        "                                   data_box.start[1]:data_box.end[1],\n",
        "                                   z:z+1, 0:1].read().result(), [3, 2, 1, 0])\n",
        "  warped.append(\n",
        "      warp.warp_subvolume(data, data_box, inv_map[:, z:z+1, ...], box1x, stride, out_box, 'lanczos', parallelism=1)[0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p2VfVXl1PKU"
      },
      "outputs": [],
      "source": [
        "warped_xyz = np.transpose(np.concatenate(warped, axis=0), [2, 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhxP4TYE26M0"
      },
      "source": [
        "Render an XZ cross section to check the quality of the alignment visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsjuVyAJ2fTv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 3))\n",
        "plt.imshow(warped_xyz[:, 500, :].T, cmap=plt.cm.Greys_r, aspect=(1000 * 11) / (200 * 25))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP46NYIA2Vj2"
      },
      "source": [
        "The results can be inspected interactively in Neuroglancer as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa4oEsbRzeih"
      },
      "outputs": [],
      "source": [
        "!pip install neuroglancer\n",
        "import neuroglancer\n",
        "\n",
        "dimensions = neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=[11, 11, 25])\n",
        "viewer = neuroglancer.Viewer()\n",
        "with viewer.txn() as s:\n",
        "  s.dimensions = dimensions\n",
        "  s.layers['em'] = neuroglancer.ImageLayer(source=neuroglancer.LocalVolume(warped_xyz, dimensions))\n",
        "\n",
        "viewer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsyPqJDIioRd"
      },
      "source": [
        "# Distributed mesh optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjXQAG9VI29X"
      },
      "source": [
        "The serial mesh optimization process described above might be too slow for large volumes. Fortunately, mesh optimization can be parallelized by splitting the volume into overlapping blocks of sections, solving the blocks independently in parallel, and reconciling the solutions to produce a globally consistent mesh.\n",
        "\n",
        "If we now represent every block as a virtual \"section\" and use solved state of the last section of every block as the \"flow field\", we can reuse the existing SOFIMA functionality to optimize a new mesh representing the blocks. This can then be combined with the in-block meshes to form a global solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXD1Z2cwrNlE"
      },
      "source": [
        "To demonstrate how this works, we will now solve the same mesh as before independently for [0..50], [50..100], [100..150], [150..199]. Note that the blocks overlap by exactly one and that in a real setting, this would be done in parallel instead of in a loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7orBbUU_aug"
      },
      "outputs": [],
      "source": [
        "from connectomics.volume import subvolume\n",
        "from sofima.processor import maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9ktaFUbQ0VS"
      },
      "outputs": [],
      "source": [
        "block1 = [np.zeros_like(final_flow[:, 0:1, ...])]\n",
        "block2 = [np.zeros_like(final_flow[:, 0:1, ...])]\n",
        "block3 = [np.zeros_like(final_flow[:, 0:1, ...])]\n",
        "block4 = [np.zeros_like(final_flow[:, 0:1, ...])]\n",
        "\n",
        "for blk, start, stop in ((block1, 0, 50), (block2, 50, 100), (block3, 100, 150), (block4, 150, final_flow.shape[1])):\n",
        "  for z in tqdm(range(start, stop)):\n",
        "    prev = map_utils.compose_maps_fast(final_flow[:, z:z+1, ...], origin, stride,\n",
        "                                       blk[-1], origin, stride)\n",
        "    x = np.zeros_like(blk[0])\n",
        "    x, e_kin, num_steps = mesh.relax_mesh(x, prev, config)\n",
        "    x = np.array(x)\n",
        "    blk.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMqoaPHYODJM"
      },
      "outputs": [],
      "source": [
        "block1 = np.concatenate(block1, axis=1)\n",
        "block2 = np.concatenate(block2, axis=1)\n",
        "block3 = np.concatenate(block3, axis=1)\n",
        "block4 = np.concatenate(block4, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPjCI1HurUmk"
      },
      "source": [
        "The mesh for the last section of 'block1' tells us where the first section of 'block2' should be, if block2 was aligned to block1. This mesh data for the last section of every block can be stacked to form a 'cross-block' flow field. We will downsample it 2x in-plane as the block-wise mesh can be coarser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNip5AEj6iRv"
      },
      "outputs": [],
      "source": [
        "map_box = bounding_box.BoundingBox(start=(0, 0, 0), size=block1.shape[1:][::-1])\n",
        "map2x_box = map_box.scale(0.5)\n",
        "xblk_stride = stride * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZejOSOCWOzIj"
      },
      "outputs": [],
      "source": [
        "xblock_flow = np.stack([block1[:, -1, ...], block2[:, -1, ...],\n",
        "                        block3[:, -1, ...], block4[:, -1, ...]], axis=1)\n",
        "xblock_flow = map_utils.resample_map(xblock_flow, map_box, map2x_box, stride, xblk_stride)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TLBIGJsMWse"
      },
      "source": [
        "With the flow field prepared, we are aready to optimize the block-wise mesh. To take into account that every mesh section represents a much thicker piece of physical tissue, we lower the value of $k_0$, making the inter-section forces weak relative to the intra-section ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpMkmzBnTpYd"
      },
      "outputs": [],
      "source": [
        "xblk_config = mesh.IntegrationConfig(dt=0.001, gamma=0.0, k0=0.001, k=0.1, stride=(xblk_stride, xblk_stride), num_iters=1000,\n",
        "                                     max_iters=100000, stop_v_max=0.005, dt_max=1000, start_cap=0.01,\n",
        "                                     final_cap=10, prefer_orig_order=True)\n",
        "\n",
        "xblk = []\n",
        "for z in tqdm(range(xblock_flow.shape[1])):\n",
        "  if z == 0:\n",
        "    prev = xblock_flow[:, z:z+1, ...]\n",
        "  else:\n",
        "    prev = map_utils.compose_maps_fast(xblock_flow[:, z:z+1, ...], origin, xblk_stride, xblk[-1], origin, xblk_stride)\n",
        "  x = np.zeros_like(xblock_flow[:, 0:1, ...])\n",
        "  x, e_kin, num_steps = mesh.relax_mesh(x, prev, xblk_config)\n",
        "  x = np.array(x)\n",
        "  xblk.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsBi8VuNor2h"
      },
      "outputs": [],
      "source": [
        "xblk = np.concatenate(xblk, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynaAF9l_mfD"
      },
      "source": [
        "The cross-block mesh can now be combined with the per-block solutions to form a globally consistent mesh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA-YeFsFcc54"
      },
      "outputs": [],
      "source": [
        "main = np.concatenate([block1[:, :50], block2[:, :50], block3[:, :50], block4], axis=1)\n",
        "last = np.zeros_like(main)\n",
        "last[:, 50, ...] = block1[:, -1, ...]\n",
        "last[:, 100, ...] = block2[:, -1, ...]\n",
        "last[:, 150, ...] = block3[:, -1, ...]\n",
        "last[:, 199, ...] = block4[:, -1, ...]\n",
        "\n",
        "main_inv = map_utils.invert_map(main, map_box, map_box, stride)\n",
        "last_inv = map_utils.invert_map(last, map_box, map_box, stride)\n",
        "xblk_upsampled = map_utils.resample_map(xblk, map2x_box, map_box, stride * 2, stride)\n",
        "xblk_inv = map_utils.invert_map(xblk_upsampled, map_box, map_box, stride)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEOT8lT9WQsw"
      },
      "outputs": [],
      "source": [
        "class ReconcileCrossBlockMaps(maps.ReconcileCrossBlockMaps):\n",
        "  def _open_volume(self, path: str):\n",
        "    if path == 'main_inv':\n",
        "      return main_inv\n",
        "    elif path == 'last_inv':\n",
        "      return last_inv\n",
        "    elif path == 'xblk':\n",
        "      return xblk_upsampled\n",
        "    elif path == 'xblk_inv':\n",
        "      return xblk_inv\n",
        "    else:\n",
        "      raise ValueError(f'Unknown volume {path}')\n",
        "\n",
        "config = maps.ReconcileCrossBlockMaps.Config(\n",
        "    cross_block='xblk',\n",
        "    cross_block_inv='xblk_inv',\n",
        "    last_inv='last_inv',\n",
        "    main_inv='main_inv',\n",
        "    z_map={'50': 0, '100': 1, '150': 2, '199': 3},\n",
        "    stride=stride,\n",
        "    xy_overlap=0)\n",
        "reconcile = ReconcileCrossBlockMaps(config)\n",
        "reconcile.set_effective_subvol_and_overlap(map_box.size, (0, 0, 0))\n",
        "main_box = bounding_box.BoundingBox(start=(0, 0, 0), size=main.shape[1:][::-1])\n",
        "global_map = reconcile.process(subvolume.Subvolume(main, main_box))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ypthgsl3jeu"
      },
      "source": [
        "We're now ready to render the aligned images..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkDpJRF3cT8O"
      },
      "outputs": [],
      "source": [
        "global_map_inv = map_utils.invert_map(global_map.data, map_box, map_box, stride)\n",
        "warped2 = [np.transpose(unaligned_1x[1000:2000, 2000:3000, 0:1, 0].read().result(), [2, 1, 0])]\n",
        "\n",
        "for z in tqdm(range(1, unaligned_1x.shape[2])):\n",
        "  data_box = bounding_box.BoundingBox(start=(500, 1500, 0), size=(2000, 2000, 1))\n",
        "  out_box = bounding_box.BoundingBox(start=(1000, 2000, 0), size=(1000, 1000, 1))\n",
        "\n",
        "  data = np.transpose(unaligned_1x[data_box.start[0]:data_box.end[0],\n",
        "                                   data_box.start[1]:data_box.end[1],\n",
        "                                   z:z+1, 0:1].read().result(), [3, 2, 1, 0])\n",
        "  warped2.append(\n",
        "      warp.warp_subvolume(data, data_box, global_map_inv[:, z:z+1, ...], box1x, stride, out_box, 'lanczos', parallelism=1)[0, ...])\n",
        "\n",
        "warped2_xyz = np.transpose(np.concatenate(warped2, axis=0), [2, 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvK6EY03nw0"
      },
      "source": [
        "... and inspect them as a static image ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HiC6LAh3eMl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 3))\n",
        "plt.imshow(warped2_xyz[:, 500, :].T, cmap=plt.cm.Greys_r, aspect=(1000 * 11) / (200 * 25))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxEi9Dt63qjp"
      },
      "source": [
        "... or interactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlwSl54lM2wI"
      },
      "outputs": [],
      "source": [
        "import neuroglancer\n",
        "\n",
        "dimensions = neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=[11, 11, 25])\n",
        "viewer = neuroglancer.Viewer()\n",
        "with viewer.txn() as s:\n",
        "  s.dimensions = dimensions\n",
        "  s.layers['em'] = neuroglancer.ImageLayer(source=neuroglancer.LocalVolume(warped2_xyz, dimensions))\n",
        "\n",
        "viewer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}